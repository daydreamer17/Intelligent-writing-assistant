# ============================================================================
# HelloAgents Áªü‰∏ÄÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆÊñá‰ª∂
# ============================================================================
# Â§çÂà∂Ê≠§Êñá‰ª∂‰∏∫ .env Âπ∂Â°´ÂÖ•‰Ω†ÁöÑAPIÂØÜÈí•
# Á≥ªÁªüË¶ÅÊ±ÇÔºöPython 3.10+ ÔºàÂøÖÈúÄÔºâ

# ============================================================================
# üöÄ Áªü‰∏ÄÈÖçÁΩÆÊ†ºÂºèÔºàÊé®ËçêÔºâ- Ê°ÜÊû∂Ëá™Âä®Ê£ÄÊµãprovider
# ============================================================================
# Âè™ÈúÄÈÖçÁΩÆ‰ª•‰∏ã4‰∏™ÈÄöÁî®ÁéØÂ¢ÉÂèòÈáè,Ê°ÜÊû∂‰ºöËá™Âä®ËØÜÂà´LLMÊèê‰æõÂïÜÔºö

LLM_PROVIDER=openai
LLM_MODEL=YOUR_MODEL_NAME
LLM_API_KEY=YOUR_API_KEY
LLM_API_BASE=YOUR_URL
LLM_TIMEOUT=300
LLM_MAX_TOKENS=1200
LLM_RETRY_MAX=5
LLM_RETRY_BACKOFF=10
LLM_COOLDOWN_SECONDS=2
LLM_CLEAR_SESSION_ON_OVERFLOW=true
LLM_MAX_INPUT_CHARS=12000
LLM_MAX_CONTEXT_TOKENS=32768
LLM_INPUT_SAFETY_MARGIN=8000
LLM_CHARS_PER_TOKEN=0.6
LLM_HISTORY_MAX_CHARS=8000
LLM_SESSION_MAX_HISTORY_MESSAGES=80
LLM_SESSION_MAX_HISTORY_CHARS=12000
LLM_MEMORY_MAX_PIN_RATIO=0.35
LLM_CONTEXT_COMPRESS_ENABLED=true
LLM_CONTEXT_COMPRESS_THRESHOLD=0.85
LLM_CONTEXT_COMPRESS_TARGET=0.5
LLM_CONTEXT_COMPRESS_KEEP_LAST=4
LLM_CONTEXT_COMPRESS_MAX_TOKENS=600
LLM_CONTEXT_COMPRESS_INPUT_CHARS=12000
LLM_CONTEXT_COMPRESS_MERGE_THRESHOLD=0.6
LLM_CONTEXT_COMPRESS_MERGE_TARGET=0.35
LLM_LAYERED_LONG_RATIO=0.35
LLM_LAYERED_RECENT_RATIO=0.25
LLM_LAYERED_TAIL_RATIO=0.4
# Guard against MCP-expanded tool descriptions exploding system prompt tokens.
LLM_TOOL_PROMPT_MAX_TOKENS=4000
LLM_TOOL_PROMPT_CONTEXT_RATIO=0.8
LLM_MAX_TOOL_CALLS_PER_RESPONSE=2
LLM_MAX_TOOL_CALLS_PER_REQUEST=4
LLM_MAX_REPEATED_TOOL_CALLS=2
LLM_MAX_TOOL_ITERATIONS=2
# Max chars per individual tool result (GitHub search can return 100K+).
LLM_MAX_TOOL_RESULT_CHARS=3000
# Enable only if you truly need agent-side function/tool calling in normal writing flow.
LLM_AGENT_TOOL_CALLING_ENABLED=false
# Max unique tool calls allowed per single LLM response (prevents small models
# from generating hundreds of identical [TOOL_CALL:...] markers).
# Max tool-call iterations (LLM -> tools -> LLM rounds) per agent.run/stream call.
# Set to 2 to allow: iteration 1 (tool calls) -> iteration 2 (final answer).
# Stage-based tool layering: assign different tool subsets to each writing stage.
LLM_STAGE_BASED_TOOLS_ENABLED=true
# Tool allowlists for each stage (comma-separated tool names, empty = no tools).
LLM_TOOLS_PLAN_STAGE=github_search_repositories,github_search_code
LLM_TOOLS_DRAFT_STAGE=github_search_repositories,github_search_code,github_get_file_contents
LLM_TOOLS_REVIEW_STAGE=github_search_repositories,github_search_code
LLM_TOOLS_REWRITE_STAGE=
LLM_TOOL_POLICY_MODE=rules
LLM_TOOL_POLICY_SEARCH_KEYWORDS=github,repo,repository,issue,pr,commit,code,readme
LLM_TOOL_POLICY_READ_KEYWORDS=owner/repo,path,README,.md,.py
LLM_TOOL_POLICY_DISABLE_WHEN_RAG_STRONG=true
LLM_MEMORY_TAG_PIN=[MEM:PIN]
LLM_MEMORY_TAG_COMPRESS=[MEM:COMPRESS]
LLM_MEMORY_TAG_DEFER=[MEM:DEFER]
LLM_MEMORY_TAG_FORGET=[MEM:FORGET]
LLM_MEMORY_META_PRIORITY=true
LLM_MEMORY_AUTO_POLICY_ENABLED=true
LLM_MEMORY_MIN_CHARS=16
LLM_MEMORY_PIN_THRESHOLD=0.82
LLM_MEMORY_COMPRESS_THRESHOLD=0.56
LLM_MEMORY_DEFER_THRESHOLD=0.30
LLM_MEMORY_RECENCY_WEIGHT=0.4
LLM_MEMORY_ROLE_WEIGHT=0.2
LLM_MEMORY_LENGTH_WEIGHT=0.1
LLM_MEMORY_KEYWORD_WEIGHT=0.3
LLM_MEMORY_LENGTH_BASE=1200
LLM_MEMORY_KEYWORD_HITS_BASE=3
LLM_MEMORY_IMPORTANT_KEYWORDS=constraint,requirements,must,style,format,citation,source,topic,goal,‰∏çË¶Å,ÂøÖÈ°ª,Á∫¶Êùü,È£éÊ†º,ÂºïÁî®,ÁõÆÊ†á
LLM_MEMORY_LOG_VERBOSE=true
LLM_COLD_STORE_ENABLED=true
LLM_COLD_STORE_PATH=data/app.db
LLM_COLD_STORE_MAX_CHARS=5000
LLM_COLD_RECALL_ENABLED=true
LLM_COLD_RECALL_TOP_K=3
LLM_COLD_RECALL_MAX_CHARS=300
LLM_COLD_RECALL_LOOKBACK=200
LLM_SESSION_MAX_AGENTS=32
LLM_SESSION_TTL_SECONDS=3600
STAGE_PLAN_MAX_TOKENS=2000
STAGE_PLAN_MAX_INPUT_CHARS=8000
STAGE_DRAFT_MAX_TOKENS=6000
STAGE_DRAFT_MAX_INPUT_CHARS=12000
STAGE_REVIEW_MAX_TOKENS=4000
STAGE_REVIEW_MAX_INPUT_CHARS=10000
STAGE_REWRITE_MAX_TOKENS=6000
STAGE_REWRITE_MAX_INPUT_CHARS=12000
PIPELINE_PLAN_MAX_TOKENS=2000
PIPELINE_PLAN_MAX_INPUT_CHARS=8000
PIPELINE_DRAFT_MAX_TOKENS=6000
PIPELINE_DRAFT_MAX_INPUT_CHARS=12000
PIPELINE_REVIEW_MAX_TOKENS=4000
PIPELINE_REVIEW_MAX_INPUT_CHARS=10000
PIPELINE_REWRITE_MAX_TOKENS=6000
PIPELINE_REWRITE_MAX_INPUT_CHARS=12000
PIPELINE_STAGE_SLEEP=3
PIPELINE_EFFECTIVE_OUTPUT_MIN_CHARS=200
RAG_HYDE_ENABLED=false
RAG_BILINGUAL_REWRITE_ENABLED=true
RAG_BILINGUAL_REWRITE_MAX_CHARS=320
RAG_BILINGUAL_REWRITE_MAX_TOKENS=96
RAG_QUERY_MAX_CHARS=800
RAG_HYDE_MAX_CHARS=1500
RAG_HYDE_MAX_TOKENS=400
RAG_MAX_EXPANSION_QUERIES=3
RAG_RERANK_ENABLED=false
RAG_RERANK_TOP_K=5
RAG_RERANK_MAX_CANDIDATES=15
RAG_RERANK_SNIPPET_CHARS=600
RAG_RERANK_MAX_PROMPT_CHARS=6000
RAG_RERANK_MAX_TOKENS=300
# Âú®Á∫øÂä®ÊÄÅÊ£ÄÁ¥¢Á≠ñÁï•Ë∑ØÁî±ÔºàÊåâ query Á±ªÂûãÂàáÊç¢ dense/rerank/hyde/bilingualÔºâ
RAG_QUERY_STRATEGY_ROUTING_ENABLED=false
RAG_DYNAMIC_TOPK_ENABLED=true
RAG_DYNAMIC_SMALL_THRESHOLD=50
RAG_DYNAMIC_LARGE_THRESHOLD=500
RAG_DYNAMIC_TOPK_SMALL=5
RAG_DYNAMIC_TOPK_MEDIUM=10
RAG_DYNAMIC_TOPK_LARGE=12
RAG_DYNAMIC_CANDIDATES_SMALL=15
RAG_DYNAMIC_CANDIDATES_MEDIUM=24
RAG_DYNAMIC_CANDIDATES_LARGE=36
RAG_NOTES_DYNAMIC_ENABLED=true
RAG_NOTES_TOP_K=7
RAG_NOTES_SMALL_THRESHOLD=10
RAG_NOTES_LARGE_THRESHOLD=100
RAG_NOTES_TOP_K_SMALL=5
RAG_NOTES_TOP_K_MEDIUM=8
RAG_NOTES_TOP_K_LARGE=12
RAG_NOTES_INJECTION_MAX_ITEMS=8
RAG_NOTES_INJECTION_MAX_CHARS=4000
RAG_NOTES_INJECTION_MIN_CHARS=120
RAG_NOTES_INJECTION_MIN_SCORE=0.03
RAG_COVERAGE_THRESHOLD=0.3
RAG_COVERAGE_SEMANTIC_ENABLED=false
RAG_COVERAGE_SEMANTIC_THRESHOLD=0.25
RAG_COVERAGE_SEMANTIC_MAX_PARAGRAPHS=20
RAG_COVERAGE_SEMANTIC_MAX_NOTES=12
RAG_COVERAGE_SEMANTIC_BATCH_SIZE=8
RAG_COVERAGE_SEMANTIC_MAX_TEXT_CHARS=2000
# Generation mode:
# - rag_only: evidence-only, no MCP external context, refuse when evidence is insufficient
# - hybrid: allow MCP/common-sense completion, add [Êé®Êñ≠] to non-evidence paragraphs
# - creative: free generation (novel/copywriting), no strict evidence/refusal/citation enforcement
#   (MCP enabled by default; set RAG_CREATIVE_MCP_ENABLED=false to disable)
#   (Conversation memory disabled by default; set RAG_CREATIVE_MEMORY_ENABLED=true to enable)
RAG_GENERATION_MODE=rag_only
RAG_CREATIVE_MCP_ENABLED=true
RAG_CREATIVE_MEMORY_ENABLED=false
RAG_REFUSAL_ENABLED=true
RAG_REFUSAL_MODE=fallback
# Keep refusal query concise for relevance scoring (avoid outline/draft dilution).
RAG_REFUSAL_QUERY_MAX_CHARS=480
RAG_REFUSAL_INCLUDE_OUTLINE=false
RAG_REFUSAL_INCLUDE_DRAFT=false
RAG_REFUSAL_MIN_QUERY_TERMS=2
RAG_REFUSAL_MIN_DOCS=1
RAG_REFUSAL_MIN_RECALL=0.1
RAG_REFUSAL_MIN_AVG_RECALL=0.05
RAG_REFUSAL_FALLBACK_TOP_K=12
RAG_REFUSAL_FALLBACK_MIN_DOCS=2
RAG_REFUSAL_FALLBACK_MIN_RECALL=0.12
RAG_REFUSAL_FALLBACK_MIN_AVG_RECALL=0.06
RAG_CITATION_ENFORCE=false
RAG_CITATION_REQUIRE_ALL_LABELS=true
RAG_CITATION_APPEND_MISSING_TO_TAIL=true
RAG_CITATION_TAIL_PREFIX=ÂºïÁî®Ë°•ÂÖ®
RAG_CITATION_TOP_K=5
RAG_HYBRID_INFERENCE_TAG=[Êé®Êñ≠]
RAG_HYBRID_MIN_PARAGRAPH_CHARS=12
RAG_EVIDENCE_MAX_ITEMS=12
RAG_EVIDENCE_MAX_CHARS=4000
RAG_EVIDENCE_MAX_TOKENS=400
MCP_GITHUB_ENABLED=false
MCP_GITHUB_TOOL_SCOPE=search
# Max number of expanded GitHub MCP tools to register (0 means unlimited).
MCP_GITHUB_MAX_TOOLS=5
GITHUB_PERSONAL_ACCESS_TOKEN=YOUR_GITHUB_TOKEN
STORAGE_PATH=data/app.db
# Preferred retrieval switch:
# - sqlite_only: use SQLite keyword retrieval only
# - hybrid: SQLite + Qdrant vector retrieval (if Qdrant is configured)
RETRIEVAL_MODE=sqlite_only
# Conversation memory switch:
# - session: isolate memory by session_id (recommended)
# - global: share one global in-memory history
CONVERSATION_MEMORY_MODE=session
# Legacy compatibility flag (derived automatically when RETRIEVAL_MODE is set):
MEMORY_MODE=short_term
QDRANT_URL=YOUR_URL
QDRANT_API_KEY=YOUR_API_KEY
QDRANT_COLLECTION=hello_agents_vectors
QDRANT_EMBED_DIM=1024
QDRANT_DISTANCE=cosine
QDRANT_TIMEOUT=30
EMBEDDING_PROVIDER=openai_compatible
EMBEDDING_MODEL=YOUR_MODEL_NAME
EMBEDDING_API_KEY=YOUR_API_KEY
EMBEDDING_API_BASE=YOUR_URL
EMBEDDING_TIMEOUT=20
UPLOAD_MAX_MB=10
UVICORN_RELOAD=false
